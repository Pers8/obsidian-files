Natural language processing (NLP) is a rapidly growing field within data science. This field has led to the development of specialized machine learning models that allow us to summarize, analyze, and classify text. These specialized models are very applicable to a variety of situations. For example, if a company wants to know more about its customers, it can use NLP to summarize and classify reviews - thus, gaining valuable insight [1]. I work in the online shopping department at my local Superstore which happens to employ NLP to discover potential improvements that can be made in our department by analyzing customer surveys. For example, if customers are dissatisfied with our selection of produce, NLP can allow us to detect this sentiment without needing a human to read every single review. Since customer sentiment can be described and communicated in so many ways (there are many ways one can word one’s dissatisfaction - such as using sarcasm), it’s almost necessary for us to use NLP if we want to get accurate insight - rather than getting humans to read each review or relying on other more traditional algorithms (such as word frequency analysis). I received part of my inspiration for this paper while working in my department. Another potential application of NLP would be if the local library wants to label each of their books based on its genre - it can use NLP models 3 to accurately [2] classify and label each book without requiring as much manual labor. Since I am an avid reader and have been looking at reading classics on my kindle, I’ve come across the Gutenberg project which makes available a lot of classic books in electronic format. The issue I saw was that each book was not labeled with an actual creation date. The date available in the metadata, was the date the book was re-released by the Gutenberg project. Thus, if I wanted to find out the actual creation date of the book I was reading, I had to refer to outside sources. This served as my main inspiration for this paper - I thought that it would be interesting to be able to label books with their creation date using NLP. One recent development in the field of NLP was the discovery that character-level convolutional neural networks (CNNs) can be applied to the problem of text-classification and yield significant performance increases - when compared with traditional methods. The structure of CNNs allow them to capture patterns in texts such as word appearance and word arrangement. This paper hopes to take advantage of CNNs and their associated performance, and to answer the question of: "To what extent are character-level CNNs viable for classifying texts by century?" - when they are created. This research question is unique when compared with prior applications - it is a rather difficult one. Firstly, there is a problem of focus. There are so many different ways the CNN might end up approaching the problem as there isn’t one surefire way to discern the century of a text - or that we’ve at least discovered. A CNN can look at the frequency of occurrence of a certain phrase such as "hereto" in order to discern between the 20th and 18th century. Or, it might notice that the 20th 4 century texts had more of a focus on the theme of "progress" rather than let’s say "liberty". Perhaps it notices that 18th century texts were more often from the perspective of a farmer rather than a factory worker. The second point of hardship is that the amount of data needed to be processed is significantly more than prior applications that have been dealing with the classification of something around the length of an article. An article is short and the meaning - if that’s what you are classifying - is somewhere in the body of the text. This application is using a dataset of texts, some having many chapters. My computing resources are not powerful enough to have the CNN read each text fully. Thus, I’ve had to shorten each text, hoping that the patterns needed to discern between the two centuries, were in the shortened version of the text. Perhaps, the CNN won’t be able to identify the prevalence of ’liberty’ in the texts of the 18th century all because I had to shorten them. Also, because my CNN (design) will be looking at characters as being the smallest unit of text, there will be many more ’features’ or attributes than instances or data points. The length of each text is 3740 characters, while there are only 2101 books. Thus there is a small dimensionality problem (often referred to as the Curse of Dimensionality). For this reason, there might not be enough data to achieve the full performance a CNN potentially has to offer. For these two reasons, my problem is rather unique and challenging. Ideally, this research question will shed more light on the limitations of current-day CNNs. To answer this research question, I will first provide a theoretical background of CNNs. Then, I will explain in more detail my chosen CNN design (sometimes referred to as architecture). Finally, I will carry out an exper5 iment to actually obtain some experimental data and figure out whether using CNNs for this application would be viable. To ensure the validity of my results, I will be using cross-validation to ensure their accuracy.